# -*- coding: utf-8 -*-
"""RAG_Diseases_txt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1luE4GOQXdTEdUjL_bhJhUTLKM--8vyM3
"""

! pip install langchain

! pip install -qU langchain-openai

import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

! pip install -qU langchain-openai

import getpass
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

!pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma

!pip install --upgrade langchain

from langchain.document_loaders import TextLoader

from langchain.document_loaders import TextLoader
from langchain.schema import Document
from langchain import hub
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Custom TextLoader with specified encoding
class CustomTextLoader(TextLoader):
    def load(self):
        try:
            with open(self.file_path, encoding='latin-1') as f:
                text = f.read()
            return [Document(page_content=text)]
        except UnicodeDecodeError:
            raise RuntimeError(f"Error loading {self.file_path}. Please check the file encoding.")

# Load the .txt file using the CustomTextLoader
file_path = "Diseases.txt"
loader = CustomTextLoader(file_path)

# Load the documents
documents = loader.load()  # Mimicking the `await loader.load()` functionality in Python
documents

# # Split the text into chunks
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
# splits = text_splitter.split_documents(documents)

# # Create vector store and retriever
# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
# retriever = vectorstore.as_retriever()

# # Create the system prompt
# system_prompt = (
#     "You are an assistant for question-answering tasks. "
#     "Use the following pieces of retrieved context to answer "
#     "the question. If you don't know the answer, say that you "
#     "don't know. Use six sentences maximum and keep the "
#     "answer concise."
#     "\n\n"
#     "{context}"
# )

# prompt = ChatPromptTemplate.from_messages(
#     [
#         ("system", system_prompt),
#         ("human", "{input}"),
#     ]
# )

# # Create question-answering chain
# question_answer_chain = create_stuff_documents_chain(llm, prompt)
# rag_chain = create_retrieval_chain(retriever, question_answer_chain)

# Assuming the document content is loaded into one Document object
full_text = documents[0].page_content

# Define what constitutes a "page" (e.g., 3000 characters per page)
page_size = 3000

# Split the full text into pages
pages = [full_text[i:i+page_size] for i in range(0, len(full_text), page_size)]

# Loop through the list of pages and display each one
for i, page in enumerate(pages, 1):
    print(f"Page {i}:\n")
    print(page)
    print("\n" + "-"*40 + "\n")

response = rag_chain.invoke({"input": "Which disease was discovered by Dr. Amara Delcroix in 2042?"})
response["answer"]

response = rag_chain.invoke({"input": "In which disease does a patient develops irregular patches of dark and light skin, creating a mottled appearance?"})
response["answer"]

response = rag_chain.invoke({"input": "what is Lanconase?"})
response["answer"]

response = rag_chain.invoke({"input": "What is the primary characteristic of Maybelline Myopathy?"})
response["answer"]

response = rag_chain.invoke({"input": "What is the incidence rate of Sephora Syndrome and Opal Osteomyelitis?"})
response["answer"] #Wrong Answer

response = rag_chain.invoke({"input": "What is the incidence rate of Sephora Syndrome?"})
response["answer"]

